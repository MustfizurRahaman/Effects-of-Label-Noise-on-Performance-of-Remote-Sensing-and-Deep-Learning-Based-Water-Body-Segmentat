{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook of Performance of Label Noise on Water Body Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_DIR = r\"C:\\Codes\\2019RGBnonwaterbodyCancelled\"\n",
    "Valid_DIR = r\"C:\\Codes\\2019RGBnonwaterbodyCancelled\\Valid\"\n",
    "Test_DIR = r\"C:\\Codes\\2019RGBnonwaterbodyCancelled\\Test\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_dir = os.path.join(Train_DIR, 'Band')\n",
    "y_train_dir = os.path.join(Train_DIR, 'Label')\n",
    "\n",
    "x_valid_dir = os.path.join(Valid_DIR, 'Band')\n",
    "y_valid_dir = os.path.join(Valid_DIR, 'Label')\n",
    "\n",
    "#x_valid_dir = os.path.join(DATA_DIR, '2017_Band2_Cropped')\n",
    "#y_valid_dir = os.path.join(DATA_DIR, '2017_Label_Cropped')\n",
    "\n",
    "x_test_dir = os.path.join(Test_DIR, 'Band')\n",
    "y_test_dir = os.path.join(Test_DIR, 'Label')\n",
    "print(x_train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "ids = os.listdir(x_train_dir)\n",
    "images_fps = [os.path.join(x_train_dir, image_id) for image_id in ids]\n",
    "print(len(images_fps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(**images):\n",
    "    n = len(images)\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    for i, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(' '.join(name.split('_')).title())\n",
    "        plt.imshow(image)\n",
    "        #print(image)\n",
    "        plt.savefig(\"img{}.jpg\".format(n))\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset as BaseDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(BaseDataset):\n",
    "    \n",
    "    \n",
    "    CLASSES = ['nonwaterbody','waterbody']\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            images_dir, \n",
    "            masks_dir, \n",
    "            classes=None, \n",
    "            augmentation=None, \n",
    "            preprocessing=None,\n",
    "    ):\n",
    "        self.ids = os.listdir(images_dir)\n",
    "        self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.ids]\n",
    "        self.masks_fps = [os.path.join(masks_dir, image_id) for image_id in self.ids]\n",
    "        \n",
    "        # convert str names to class values on masks\n",
    "        self.class_values = [self.CLASSES.index(cls.lower()) for cls in classes]\n",
    "        print(self.class_values)\n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        # read data\n",
    "        image= io.imread(self.images_fps[i])\n",
    "        #image = cv2.imread(self.images_fps[i])\n",
    "        image = cv2.cvtColor(image.astype('uint8') * 255, cv2.COLOR_BGR2RGB)\n",
    "        #image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        mask = io.imread(self.masks_fps[i], 0)\n",
    "        #print(image.shape)\n",
    "        # extract certain classes from mask (e.g. cars)\n",
    "#         mask[mask<128] = 0 # For diffused boudnaries\n",
    "#         mask[mask>=128] = 1\n",
    "\n",
    "        masks = [(mask == v) for v in self.class_values]\n",
    "        mask = np.stack(masks, axis=-1).astype('float')\n",
    "        \n",
    "        # apply augmentations\n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image, mask=mask)\n",
    "            image, mask = image, sample['mask']\n",
    "        \n",
    "        # apply preprocessing\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing( image=image,mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "        #print(mask.shape) \n",
    "        return image, mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = Dataset(x_train_dir, y_train_dir, classes=['waterbody'])\n",
    "# print(len(dataset))\n",
    "\n",
    "# image, mask = dataset[250] # get some sample\n",
    "# visualize(\n",
    "#     image=image, \n",
    "#     cars_mask=mask.squeeze(),\n",
    "# )\n",
    "\n",
    "dataset = Dataset(x_train_dir, y_train_dir,classes=['waterbody'])\n",
    "#print(len(dataset))\n",
    "\n",
    "# image, mask = dataset[69] # get some sample\n",
    "# visualize(\n",
    "#     image=image, \n",
    "#     cars_mask=mask.squeeze(),\n",
    "# )\n",
    "# dataset\n",
    "\n",
    "for i in range(5):\n",
    "    num = 20+i\n",
    "    image,mask = dataset[num]\n",
    "    visualize(\n",
    "        image = image,\n",
    "        ground_truthlabel = mask.squeeze()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import segmentation_models_pytorch as smp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCODER = 'se_resnext50_32x4d'\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "CLASSES = ['waterbody']\n",
    "ACTIVATION = 'sigmoid' # could be None for logits or 'softmax2d' for multicalss segmentation\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "# create segmentation model with pretrained encoder\n",
    "model = smp.Unet(\n",
    "    encoder_name=ENCODER, \n",
    "    encoder_weights=ENCODER_WEIGHTS, \n",
    "    classes=len(CLASSES), \n",
    "    activation=ACTIVATION,\n",
    ")\n",
    "\n",
    "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import albumentations as albu\n",
    "albumvalue=96\n",
    "def get_training_augmentation():\n",
    "    train_transform = [\n",
    "        \n",
    "#        albu.HorizontalFlip(p=0.5),\n",
    "\n",
    "#         albu.ShiftScaleRotate(scale_limit=0.5, rotate_limit=0, shift_limit=0.1, p=1, border_mode=0),\n",
    "\n",
    "#         albu.PadIfNeeded(min_height=albumvalue, min_width=albumvalue, always_apply=True, border_mode=0),\n",
    "#         albu.RandomCrop(height=albumvalue, width=albumvalue, always_apply=True),\n",
    "\n",
    "         albu.IAAAdditiveGaussianNoise(p=0.7),\n",
    "#         albu.IAAPerspective(p=0.5),\n",
    "\n",
    "#         albu.OneOf(\n",
    "#             [\n",
    "#                 albu.CLAHE(p=1),\n",
    "#                 albu.RandomBrightness(p=1),\n",
    "#                 albu.RandomGamma(p=1),\n",
    "#             ],\n",
    "#             p=0.9,\n",
    "#         ),\n",
    "\n",
    "#         albu.OneOf(\n",
    "#             [\n",
    "#                 albu.IAASharpen(p=1),\n",
    "#                 albu.Blur(blur_limit=3, p=1),\n",
    "#                 albu.MotionBlur(blur_limit=3, p=1),\n",
    "#             ],\n",
    "#             p=0.9,\n",
    "#         ),\n",
    "\n",
    "#         albu.OneOf(\n",
    "#             [\n",
    "#                 albu.RandomContrast(p=1),\n",
    "#                 albu.HueSaturationValue(p=1),\n",
    "#             ],\n",
    "#             p=0.9,\n",
    "#         ),\n",
    "    ]\n",
    "    return albu.Compose(train_transform)\n",
    "\n",
    "\n",
    "def get_validation_augmentation():\n",
    "    \n",
    "    test_transform = [\n",
    "        albu.PadIfNeeded(albumvalue, albumvalue)\n",
    "    ]\n",
    "    return albu.Compose(test_transform)\n",
    "\n",
    "\n",
    "def to_tensor(x, **kwargs):\n",
    "    return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "\n",
    "def get_preprocessing(preprocessing_fn):\n",
    "    \n",
    "    \n",
    "    _transform = [\n",
    "        albu.Lambda(image=preprocessing_fn),\n",
    "        albu.Lambda(image=to_tensor, mask=to_tensor),\n",
    "    ]\n",
    "    return albu.Compose(_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_dataset = Dataset(\n",
    "    x_train_dir, \n",
    "    y_train_dir, \n",
    "    augmentation=get_training_augmentation(), \n",
    "    classes=['waterbody'],\n",
    ")\n",
    "\n",
    "# same image with different random transforms\n",
    "for i in range(3):\n",
    "    image, mask = augmented_dataset[20]\n",
    "    visualize(image=image, mask=mask.squeeze(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(\n",
    "    x_train_dir, \n",
    "    y_train_dir, \n",
    "    augmentation=get_training_augmentation(), \n",
    "    preprocessing=get_preprocessing(preprocessing_fn),\n",
    "    classes=CLASSES,\n",
    ")\n",
    "\n",
    "valid_dataset = Dataset(\n",
    "    x_valid_dir, \n",
    "    y_valid_dir, \n",
    "    augmentation=get_validation_augmentation(), \n",
    "    preprocessing=get_preprocessing(preprocessing_fn),\n",
    "    classes=CLASSES,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dice/F1 score - https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient\n",
    "# IoU/Jaccard score - https://en.wikipedia.org/wiki/Jaccard_index\n",
    "\n",
    "loss = smp.utils.losses.DiceLoss()\n",
    "metrics = [\n",
    "    smp.utils.metrics.IoU(threshold=0.5),\n",
    "    smp.utils.metrics.Fscore(threshold=0.5),\n",
    "    smp.utils.metrics.Precision(threshold=0.5),\n",
    "    smp.utils.metrics.Recall(threshold=0.5),\n",
    "    smp.utils.metrics.Accuracy(threshold=0.5),\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.Adam([ \n",
    "    dict(params=model.parameters(), lr=0.0001),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create epoch runners \n",
    "# it is a simple loop of iterating over dataloader`s samples\n",
    "train_epoch = smp.utils.train.TrainEpoch(\n",
    "    model,\n",
    "    metrics=metrics,\n",
    "    loss=loss,  \n",
    "    optimizer=optimizer,\n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "valid_epoch = smp.utils.train.ValidEpoch(\n",
    "    model, \n",
    "    metrics=metrics, \n",
    "    loss=loss, \n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")\n",
    "#print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train model for 40 epochs\n",
    "\n",
    "max_score = 0\n",
    "epochnum=[]\n",
    "trainiouscores=[]\n",
    "traindiceloss=[]\n",
    "trainfscore=[]\n",
    "trainprecisions=[]\n",
    "trainrecalls=[]\n",
    "trainaccuracy=[]\n",
    "\n",
    "validiouscores=[]\n",
    "validdiceloss=[]\n",
    "validfscore=[]\n",
    "validprecisions=[]\n",
    "validrecalls=[]\n",
    "validaccuracy=[]\n",
    "\n",
    "for i in range(0, 5):\n",
    "    \n",
    "    print('\\nEpoch: {}'.format(i))\n",
    "    train_logs = train_epoch.run(train_loader)\n",
    "    valid_logs = valid_epoch.run(valid_loader)\n",
    "    \n",
    "    epochnum.append(i)\n",
    "    \n",
    "    ###Train Scores\n",
    "    trainiouscores.append(train_logs['iou_score'])\n",
    "    traindiceloss.append(train_logs['dice_loss'])\n",
    "    trainfscore.append(train_logs['fscore'])\n",
    "    trainprecisions.append(train_logs['precision'])\n",
    "    trainrecalls.append(train_logs['recall'])\n",
    "    trainaccuracy.append(train_logs['accuracy'])\n",
    "    \n",
    "    ###Valid Scores\n",
    "    validiouscores.append(valid_logs['iou_score'])\n",
    "    validdiceloss.append(valid_logs['dice_loss'])\n",
    "    validfscore.append(valid_logs['fscore'])\n",
    "    validprecisions.append(valid_logs['precision'])\n",
    "    validrecalls.append(valid_logs['recall'])\n",
    "    validaccuracy.append(valid_logs['accuracy'])\n",
    "    \n",
    "    \n",
    "    # do something (save model, change lr, etc.)\n",
    "    if max_score < valid_logs['accuracy']:\n",
    "        max_score = valid_logs['accuracy']\n",
    "        torch.save(model, './best_model.pth')\n",
    "        print('Model saved!')\n",
    "        \n",
    "    if i == 20:\n",
    "        optimizer.param_groups[0]['lr'] = 1e-5\n",
    "        print('Decrease decoder learning rate to 1e-5!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainaccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochnum, trainaccuracy) \n",
    "plt.plot(epochnum,validaccuracy)\n",
    "  \n",
    "# naming the x axis \n",
    "plt.xlabel('Number of epochs') \n",
    "# naming the y axis \n",
    "plt.ylabel('Accuracy') \n",
    "  \n",
    "# giving a title to my graph \n",
    "plt.title('Training and Validation Accuracy') \n",
    "plt.legend([\"Train\",\"Valid\"],loc =\"upper right\")\n",
    "plt.savefig(\"Training and Validation Accuracy.jpg\")\n",
    "# function to show the plot \n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochnum, traindiceloss) \n",
    "plt.plot(epochnum,validdiceloss)\n",
    "# naming the x axis \n",
    "plt.xlabel('Number of epochs') \n",
    "# naming the y axis \n",
    "plt.ylabel('Dice Loss') \n",
    "  \n",
    "# giving a title to my graph \n",
    "plt.title('Training and Validation Loss') \n",
    "plt.legend([\"Train\",\"Valid\"],loc =\"upper right\")  \n",
    "\n",
    "plt.savefig(\"Training and Validation Loss.jpg\")\n",
    "# function to show the plot \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochnum, trainfscore) \n",
    "plt.plot(epochnum,validfscore)  \n",
    "# naming the x axis \n",
    "plt.xlabel('Number of epochs') \n",
    "# naming the y axis \n",
    "plt.ylabel('F Score') \n",
    "  \n",
    "# giving a title to my graph \n",
    "plt.title('Training and Validation Fscore') \n",
    "plt.legend([\"Train\",\"Valid\"],loc =\"upper right\")  \n",
    "plt.savefig(\"Training and Validation Fscore.jpg\")\n",
    "\n",
    "# function to show the plot \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochnum, trainiouscores) \n",
    "plt.plot(epochnum,validiouscores)  \n",
    "# naming the x axis \n",
    "plt.xlabel('Number of epochs') \n",
    "# naming the y axis \n",
    "plt.ylabel('IoU Score') \n",
    "  \n",
    "# giving a title to my graph \n",
    "plt.title('Training and Validation IoU') \n",
    "plt.legend([\"Train\",\"Valid\"],loc =\"upper right\")  \n",
    "plt.savefig(\"Training and Validation IoU.jpg\")\n",
    "# function to show the plot \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochnum, trainprecisions) \n",
    "plt.plot(epochnum,validprecisions)  \n",
    "# naming the x axis \n",
    "plt.xlabel('Number of epochs') \n",
    "# naming the y axis \n",
    "plt.ylabel('Precision Score') \n",
    "  \n",
    "# giving a title to my graph \n",
    "plt.title('Training and Validation Precision Score') \n",
    "plt.legend([\"Train\",\"Valid\"],loc =\"upper right\")  \n",
    "plt.savefig(\"Training and Validation Precision Score.jpg\")\n",
    "# function to show the plot \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochnum, trainrecalls) \n",
    "plt.plot(epochnum,validrecalls)  \n",
    "# naming the x axis \n",
    "plt.xlabel('Number of epochs') \n",
    "# naming the y axis \n",
    "plt.ylabel('Recall Score') \n",
    "  \n",
    "# giving a title to my graph \n",
    "plt.title('Training and Validation Recall Score') \n",
    "plt.legend([\"Train\",\"Valid\"],loc =\"upper right\")  \n",
    "plt.savefig(\"Training and Validation Recall Score.jpg\")\n",
    "# function to show the plot \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = torch.load('./best_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = Dataset(\n",
    "    x_test_dir, \n",
    "    y_test_dir, \n",
    "    augmentation=get_validation_augmentation(), \n",
    "    preprocessing=get_preprocessing(preprocessing_fn),\n",
    "    classes=CLASSES,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image,mask = test_dataset[61]\n",
    "# visualize(\n",
    "#     image = image,\n",
    "#     ground_truthlabel = mask.squeeze()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_epoch = smp.utils.train.ValidEpoch(\n",
    "    model=best_model.cuda(),\n",
    "    loss=loss.cuda(),\n",
    "    metrics=metrics,\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "logs = test_epoch.run(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_vis = Dataset(\n",
    "    x_test_dir, y_test_dir, \n",
    "    classes=CLASSES,\n",
    ")\n",
    "\n",
    "\n",
    "test_temp_dataset = Dataset(\n",
    "    x_test_dir, \n",
    "    y_test_dir, \n",
    "    #augmentation=get_validation_augmentation(), \n",
    "    #preprocessing=get_preprocessing(preprocessing_fn),\n",
    "    classes=CLASSES,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(30):\n",
    "    n = np.random.choice(len(test_dataset))\n",
    "    #n=23\n",
    "    image_vis = test_dataset_vis[n][0].astype('uint8')\n",
    "    image, gt_mask = test_dataset[n]\n",
    "    tempimage, tempgt_mask = test_temp_dataset[n]\n",
    "    \n",
    "    gt_mask = gt_mask.squeeze()\n",
    "    \n",
    "    x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n",
    "    pr_mask = best_model.predict(x_tensor)\n",
    "    pr_mask = (pr_mask.squeeze().cpu().numpy().round())\n",
    "        \n",
    "    visualize(\n",
    "        image=image_vis, \n",
    "        ground_truth_mask=tempgt_mask, \n",
    "        predicted_mask=pr_mask\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(epochnum)):\n",
    "    print(\"Epoch \",i,\" trainacc \",trainaccuracy[i],\" trainiou \",trainiouscores[i],\" traindice \",traindiceloss[i],\" trainfscore \",trainfscore[i],\" trainprecisions \",trainprecisions[i],\" trainrecall \",trainrecalls[i],\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainiouscores=[]\n",
    "traindiceloss=[]\n",
    "trainfscore=[]\n",
    "trainprecisions=[]\n",
    "trainrecalls=[]\n",
    "trainaccuracy=[]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
